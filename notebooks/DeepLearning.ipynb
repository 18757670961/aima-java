{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as the supporting material for the chapter **Deep Learning**. In this notebook, we'll learn different activation funtions. Then we'll create a deep neural network using Deeplearning4j and train a model capable of classifying random handwriting digits. \n",
    "\n",
    ">_\"While handwriting recognition has been attempted by different machine learning algorithms over the years, deep learning performs remarkably well and achieves an accuracy of over 99.7% on the MNIST dataset.\"_ \n",
    "\n",
    "So, let's begin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addc3f2e-980a-4df4-b0eb-946d7393d8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.nd4j nd4j-native-platform 0.9.1\n",
    "org.deeplearning4j deeplearning4j-core 0.9.1\n",
    "org.datavec datavec-api 0.9.1\n",
    "org.datavec datavec-local 0.9.1\n",
    "org.datavec datavec-dataframe 0.9.1\n",
    "org.bytedeco javacpp 1.5\n",
    "org.apache.httpcomponents httpclient 4.3.5\n",
    "org.deeplearning4j deeplearning4j-ui_2.11 0.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "### 1.) Saturating activation funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea5f537-d200-4a46-ba09-1a72a43cb6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.nd4j.linalg.api.ndarray.INDArray;\n",
    "import org.nd4j.linalg.factory.Nd4j;\n",
    "import org.nd4j.linalg.ops.transforms.Transforms;\n",
    "import org.nd4j.linalg.api.iter.NdIndexIterator;\n",
    "\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "\n",
    "INDArray array = Nd4j.linspace(-5,5,200);\n",
    "INDArray sigmoid = Transforms.sigmoid(array);\n",
    "\n",
    "def ch = new Crosshair(color: Color.gray, width: 2, style: StrokeType.DOT);\n",
    "p1 = new Plot(title: \"Sigmoid activation function\", crosshair: ch);\n",
    "p1 << new ConstantLine(x: 0, y: 0, color: Color.black);\n",
    "p1 << new ConstantLine(y: 1, color: Color.black, style: StrokeType.DOT);\n",
    "p1 << new Line(x: [-5, 5], y: [-3/4, 7/4], style: StrokeType.DASH, color: Color.green);\n",
    "p1 << new Line(x: toDoubleArrayList(array), y: toDoubleArrayList(sigmoid), displayName: \"Sigmoid\", color: Color.blue, width: 3);\n",
    "p1 << new Text(x: 0, y: 0.5, text: \"Linear\", pointerAngle: 3.505);\n",
    "p1 << new Text(x: -5, y: 0, text: \"Saturating\", pointerAngle: 1.57);\n",
    "p1 << new Text(x: 5, y: 1, text: \"Saturating\", pointerAngle: 4.71);\n",
    "\n",
    "public List<Double> toDoubleArrayList(INDArray array){\n",
    "    NdIndexIterator iter = new NdIndexIterator(200);\n",
    "    List<Double> list = new ArrayList<Double>();\n",
    "    while (iter.hasNext()) {\n",
    "        int[] nextIndex = iter.next();\n",
    "        double nextVal = array.getDouble(nextIndex);\n",
    "        list.add(nextVal);\n",
    "    }\n",
    "    return list;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6424db51-267d-4054-92a0-9f7eb3c5ee6e",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.nd4j.linalg.api.ndarray.INDArray;\n",
    "import org.nd4j.linalg.factory.Nd4j;\n",
    "import org.nd4j.linalg.ops.transforms.Transforms;\n",
    "import org.nd4j.linalg.api.iter.NdIndexIterator;\n",
    "\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "\n",
    "INDArray array = Nd4j.linspace(-5,5,200);\n",
    "INDArray relu = Transforms.relu(array);\n",
    "INDArray leakyRelu = Transforms.leakyRelu(array);\n",
    "INDArray elu = Transforms.elu(array);\n",
    "\n",
    "def ch = new Crosshair(color: Color.gray, width: 2, style: StrokeType.DOT);\n",
    "p1 = new Plot(title: \"Non saturating activation function\", crosshair: ch);\n",
    "p1 << new ConstantLine(x: 0, y: 0, color: Color.black);\n",
    "p1 << new ConstantLine(y: -1, color: Color.black, style: StrokeType.DOT);\n",
    "p1.getYAxes()[0].setBound(-1.5,5);\n",
    "p1 << new Line(x: toDoubleArrayList(array), y: toDoubleArrayList(elu), displayName: \"ELU (Î±=1)\", color: Color.red)\n",
    "p1 << new Line(x: toDoubleArrayList(array), y: toDoubleArrayList(relu), displayName: \"ReLU\", color: Color.orange)\n",
    "p1 << new Line(x: toDoubleArrayList(array), y: toDoubleArrayList(leakyRelu), displayName: \"Leaky ReLU\", color: Color.blue);\n",
    "p1 << new Text(x: -5, y: 0, text: \"Leak\", pointerAngle: 1.57);\n",
    "\n",
    "\n",
    "public List<Double> toDoubleArrayList(INDArray array){\n",
    "    NdIndexIterator iter = new NdIndexIterator(200);\n",
    "    List<Double> list = new ArrayList<Double>();\n",
    "    while (iter.hasNext()) {\n",
    "        int[] nextIndex = iter.next();\n",
    "        double nextVal = array.getDouble(nextIndex);\n",
    "        list.add(nextVal);\n",
    "    }\n",
    "    return list;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on MNIST using the Leaky ReLU. \n",
    "\n",
    "We've to create a DataUtils class first, containing methods required for downloading, extracting and deleting the dataset files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package aima.notebooks.deeplearning;\n",
    "\n",
    "import org.apache.commons.compress.archivers.tar.TarArchiveEntry;\n",
    "import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;\n",
    "import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;\n",
    "import org.apache.http.HttpEntity;\n",
    "import org.apache.http.client.methods.CloseableHttpResponse;\n",
    "import org.apache.http.client.methods.HttpGet;\n",
    "import org.apache.http.impl.client.CloseableHttpClient;\n",
    "import org.apache.http.impl.client.HttpClientBuilder;\n",
    "\n",
    "import java.io.*;\n",
    "import java.nio.file.*;\n",
    "import java.nio.file.attribute.BasicFileAttributes;\n",
    "\n",
    "public class DataUtils{\n",
    "    \n",
    "    public DataUtils(){}\n",
    "    \n",
    "    public boolean downloadFile(String remoteUrl, String localPath) throws IOException {\n",
    "        boolean downloaded = false;\n",
    "        if (remoteUrl == null || localPath == null)\n",
    "            return downloaded;\n",
    "        File file = new File(localPath);\n",
    "        if (!file.exists()) {\n",
    "            file.getParentFile().mkdirs();\n",
    "            HttpClientBuilder builder = HttpClientBuilder.create();\n",
    "            CloseableHttpClient client = builder.build();\n",
    "            try {\n",
    "                CloseableHttpResponse response = client.execute(new HttpGet(remoteUrl))\n",
    "                HttpEntity entity = response.getEntity();\n",
    "                if (entity != null) {\n",
    "                    try {\n",
    "                        FileOutputStream outstream = new FileOutputStream(file)\n",
    "                        entity.writeTo(outstream);\n",
    "                        outstream.flush();\n",
    "                        outstream.close();\n",
    "                    } catch(IOException e){\n",
    "                        System.out.println(e);\n",
    "                    }\n",
    "                }\n",
    "            } catch(IOException e){\n",
    "                System.out.println(e);\n",
    "            }\n",
    "            downloaded = true;\n",
    "        }\n",
    "        if (!file.exists())\n",
    "            throw new IOException(\"File doesn't exist: \" + localPath);\n",
    "        return downloaded;\n",
    "    }\n",
    "    public void extractTarGz(String inputPath, String outputPath) throws IOException {\n",
    "        if (inputPath == null || outputPath == null)\n",
    "            return;\n",
    "        final int bufferSize = 4096;\n",
    "        if (!outputPath.endsWith(\"\" + File.separatorChar))\n",
    "            outputPath = outputPath + File.separatorChar;\n",
    "        try {\n",
    "            TarArchiveInputStream tais = new TarArchiveInputStream(new GzipCompressorInputStream(new BufferedInputStream(new FileInputStream(inputPath))))\n",
    "            TarArchiveEntry entry;\n",
    "            while ((entry = (TarArchiveEntry) tais.getNextEntry()) != null) {\n",
    "                if (entry.isDirectory()) {\n",
    "                    new File(outputPath + entry.getName()).mkdirs();\n",
    "                } else {\n",
    "                    int count;\n",
    "                    byte[] data = new byte[bufferSize];\n",
    "                    FileOutputStream fos = new FileOutputStream(outputPath + entry.getName());\n",
    "                    BufferedOutputStream dest = new BufferedOutputStream(fos, bufferSize);\n",
    "                    while ((count = tais.read(data, 0, bufferSize)) != -1) {\n",
    "                        dest.write(data, 0, count);\n",
    "                    }\n",
    "                    dest.close();\n",
    "                }\n",
    "            }\n",
    "        } catch(IOException e){\n",
    "            System.out.println(e);\n",
    "        }\n",
    "    }\n",
    "    public void deleteDir(String path) throws IOException{\n",
    "        Path directory = Paths.get(path);\n",
    "        Files.walkFileTree(directory, new SimpleFileVisitor<Path>() {\n",
    "            @Override\n",
    "            public FileVisitResult visitFile(Path file, BasicFileAttributes attributes) throws IOException {\n",
    "                Files.delete(file); // this will work because it's always a File\n",
    "                return FileVisitResult.CONTINUE;\n",
    "            }\n",
    "\n",
    "            @Override\n",
    "            public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException {\n",
    "                Files.delete(dir); //this will work because Files in the directory are already deleted\n",
    "                return FileVisitResult.CONTINUE;\n",
    "            }\n",
    "        });\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aima.notebooks.deeplearning.DataUtils;\n",
    "import java.io.File;\n",
    "\n",
    "String DATA_URL = \"http://github.com/myleott/mnist_png/raw/master/mnist_png.tar.gz\";\n",
    "String BASE_PATH = \"./assets\";\n",
    "String localFilePath = BASE_PATH + \"/mnist_png.tar.gz\";\n",
    "DataUtils dataUtils = new DataUtils();\n",
    "if (!new File(localFilePath).exists()) {\n",
    "    if (dataUtils.downloadFile(DATA_URL, localFilePath)) {\n",
    "        dataUtils.extractTarGz(localFilePath, BASE_PATH);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Evaluation Stats*********\n",
      "\n",
      "Examples labeled as 0 classified by model as 0: 960 times\n",
      "Examples labeled as 0 classified by model as 2: 3 times\n",
      "Examples labeled as 0 classified by model as 3: 2 times\n",
      "Examples labeled as 0 classified by model as 4: 2 times\n",
      "Examples labeled as 0 classified by model as 5: 1 times\n",
      "Examples labeled as 0 classified by model as 6: 2 times\n",
      "Examples labeled as 0 classified by model as 7: 5 times\n",
      "Examples labeled as 0 classified by model as 8: 2 times\n",
      "Examples labeled as 0 classified by model as 9: 3 times\n",
      "Examples labeled as 1 classified by model as 1: 1124 times\n",
      "Examples labeled as 1 classified by model as 2: 2 times\n",
      "Examples labeled as 1 classified by model as 3: 1 times\n",
      "Examples labeled as 1 classified by model as 4: 1 times\n",
      "Examples labeled as 1 classified by model as 5: 1 times\n",
      "Examples labeled as 1 classified by model as 6: 3 times\n",
      "Examples labeled as 1 classified by model as 8: 3 times\n",
      "Examples labeled as 2 classified by model as 0: 3 times\n",
      "Examples labeled as 2 classified by model as 1: 2 times\n",
      "Examples labeled as 2 classified by model as 2: 1009 times\n",
      "Examples labeled as 2 classified by model as 3: 3 times\n",
      "Examples labeled as 2 classified by model as 4: 2 times\n",
      "Examples labeled as 2 classified by model as 6: 2 times\n",
      "Examples labeled as 2 classified by model as 7: 5 times\n",
      "Examples labeled as 2 classified by model as 8: 5 times\n",
      "Examples labeled as 2 classified by model as 9: 1 times\n",
      "Examples labeled as 3 classified by model as 2: 10 times\n",
      "Examples labeled as 3 classified by model as 3: 978 times\n",
      "Examples labeled as 3 classified by model as 5: 7 times\n",
      "Examples labeled as 3 classified by model as 7: 7 times\n",
      "Examples labeled as 3 classified by model as 8: 4 times\n",
      "Examples labeled as 3 classified by model as 9: 4 times\n",
      "Examples labeled as 4 classified by model as 2: 6 times\n",
      "Examples labeled as 4 classified by model as 4: 971 times\n",
      "Examples labeled as 4 classified by model as 6: 1 times\n",
      "Examples labeled as 4 classified by model as 8: 2 times\n",
      "Examples labeled as 4 classified by model as 9: 2 times\n",
      "Examples labeled as 5 classified by model as 0: 4 times\n",
      "Examples labeled as 5 classified by model as 2: 1 times\n",
      "Examples labeled as 5 classified by model as 3: 14 times\n",
      "Examples labeled as 5 classified by model as 4: 4 times\n",
      "Examples labeled as 5 classified by model as 5: 855 times\n",
      "Examples labeled as 5 classified by model as 6: 6 times\n",
      "Examples labeled as 5 classified by model as 7: 1 times\n",
      "Examples labeled as 5 classified by model as 8: 5 times\n",
      "Examples labeled as 5 classified by model as 9: 2 times\n",
      "Examples labeled as 6 classified by model as 0: 5 times\n",
      "Examples labeled as 6 classified by model as 1: 2 times\n",
      "Examples labeled as 6 classified by model as 2: 4 times\n",
      "Examples labeled as 6 classified by model as 3: 1 times\n",
      "Examples labeled as 6 classified by model as 4: 6 times\n",
      "Examples labeled as 6 classified by model as 5: 5 times\n",
      "Examples labeled as 6 classified by model as 6: 931 times\n",
      "Examples labeled as 6 classified by model as 7: 1 times\n",
      "Examples labeled as 6 classified by model as 8: 3 times\n",
      "Examples labeled as 7 classified by model as 1: 10 times\n",
      "Examples labeled as 7 classified by model as 2: 15 times\n",
      "Examples labeled as 7 classified by model as 3: 4 times\n",
      "Examples labeled as 7 classified by model as 4: 4 times\n",
      "Examples labeled as 7 classified by model as 5: 1 times\n",
      "Examples labeled as 7 classified by model as 7: 972 times\n",
      "Examples labeled as 7 classified by model as 8: 1 times\n",
      "Examples labeled as 7 classified by model as 9: 21 times\n",
      "Examples labeled as 8 classified by model as 0: 3 times\n",
      "Examples labeled as 8 classified by model as 2: 6 times\n",
      "Examples labeled as 8 classified by model as 3: 11 times\n",
      "Examples labeled as 8 classified by model as 4: 8 times\n",
      "Examples labeled as 8 classified by model as 5: 4 times\n",
      "Examples labeled as 8 classified by model as 6: 4 times\n",
      "Examples labeled as 8 classified by model as 7: 7 times\n",
      "Examples labeled as 8 classified by model as 8: 923 times\n",
      "Examples labeled as 8 classified by model as 9: 8 times\n",
      "Examples labeled as 9 classified by model as 0: 3 times\n",
      "Examples labeled as 9 classified by model as 1: 7 times\n",
      "Examples labeled as 9 classified by model as 2: 1 times\n",
      "Examples labeled as 9 classified by model as 3: 9 times\n",
      "Examples labeled as 9 classified by model as 4: 32 times\n",
      "Examples labeled as 9 classified by model as 5: 3 times\n",
      "Examples labeled as 9 classified by model as 7: 4 times\n",
      "Examples labeled as 9 classified by model as 8: 1 times\n",
      "Examples labeled as 9 classified by model as 9: 949 times\n",
      "\n",
      "\n",
      "==========================Scores========================================\n",
      " # of classes:    10\n",
      " Accuracy:        0.9672\n",
      " Precision:       0.9674\n",
      " Recall:          0.9669\n",
      " F1 Score:        0.9670\n",
      "Precision, recall & F1: macro-averaged (equally weighted avg. of 10 classes)\n",
      "========================================================================\n",
      "********Example finished*********\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.datavec.api.io.labels.ParentPathLabelGenerator;\n",
    "import org.datavec.api.split.FileSplit;\n",
    "import org.datavec.image.loader.NativeImageLoader;\n",
    "import org.datavec.image.recordreader.ImageRecordReader;\n",
    "import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator;\n",
    "import org.deeplearning4j.eval.Evaluation;\n",
    "import org.deeplearning4j.optimize.listeners.EvaluativeListener;\n",
    "import org.deeplearning4j.optimize.listeners.ScoreIterationListener;\n",
    "import org.deeplearning4j.optimize.api.InvocationType;\n",
    "import org.deeplearning4j.nn.api.OptimizationAlgorithm;\n",
    "import org.deeplearning4j.nn.conf.MultiLayerConfiguration;\n",
    "import org.deeplearning4j.nn.conf.NeuralNetConfiguration;\n",
    "import org.deeplearning4j.nn.conf.Updater;\n",
    "import org.deeplearning4j.nn.conf.inputs.InputType;\n",
    "import org.deeplearning4j.nn.conf.layers.DenseLayer;\n",
    "import org.deeplearning4j.nn.conf.layers.OutputLayer;\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;\n",
    "import org.deeplearning4j.nn.weights.WeightInit;\n",
    "import org.deeplearning4j.api.storage.StatsStorage;\n",
    "import org.deeplearning4j.ui.api.UIServer;\n",
    "import org.deeplearning4j.ui.stats.StatsListener;\n",
    "import org.deeplearning4j.ui.storage.FileStatsStorage;\n",
    "import org.nd4j.linalg.activations.Activation;\n",
    "import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;\n",
    "import org.nd4j.linalg.dataset.api.preprocessor.DataNormalization;\n",
    "import org.nd4j.linalg.dataset.api.preprocessor.ImagePreProcessingScaler;\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions;\n",
    "\n",
    "import java.io.File;\n",
    "import java.util.Random;\n",
    "\n",
    "int seed = 123;\n",
    "double learningRate = 0.01;\n",
    "int batchSize = 100;\n",
    "int numEpochs = 1;\n",
    "\n",
    "int height = 28;\n",
    "int width = 28;\n",
    "int channels = 1;\n",
    "int numInput = height * width;\n",
    "int numHidden = 1000;\n",
    "int numOutput = 10;\n",
    "\n",
    "//Prepare data for loading\n",
    "File trainData = new File(\"./assets/mnist_png/training\");\n",
    "FileSplit trainSplit = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, new Random(seed));\n",
    "ParentPathLabelGenerator labelMaker = new ParentPathLabelGenerator(); // use parent directory name as the image label\n",
    "ImageRecordReader trainRR = new ImageRecordReader(height, width, 1, labelMaker);\n",
    "trainRR.initialize(trainSplit);\n",
    "DataSetIterator trainIter = new RecordReaderDataSetIterator(trainRR, batchSize, 1, numOutput);\n",
    "DataNormalization imageScaler = new ImagePreProcessingScaler();\n",
    "imageScaler.fit(trainIter);\n",
    "trainIter.setPreProcessor(imageScaler);\n",
    "\n",
    "\n",
    "File testData = new File(\"./assets/mnist_png/testing\");\n",
    "FileSplit testSplit = new FileSplit(testData, NativeImageLoader.ALLOWED_FORMATS, new Random(seed));\n",
    "ImageRecordReader testRR = new ImageRecordReader(height, width, 1, labelMaker);\n",
    "testRR.initialize(testSplit);\n",
    "DataSetIterator testIter = new RecordReaderDataSetIterator(testRR, batchSize, 1, numOutput);\n",
    "testIter.setPreProcessor(imageScaler);\n",
    "\n",
    "//Build the neural network\n",
    "MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n",
    "        .seed(seed)\n",
    "        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n",
    "        .updater(Updater.ADAM)\n",
    "        .list()\n",
    "        .layer(0, new DenseLayer.Builder()\n",
    "                .nIn(numInput)\n",
    "                .nOut(numHidden)\n",
    "                .activation(Activation.RELU)\n",
    "                .weightInit(WeightInit.XAVIER)\n",
    "                .build())\n",
    "        .layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n",
    "                .nIn(numHidden)\n",
    "                .nOut(numOutput)\n",
    "                .activation(Activation.SOFTMAX)\n",
    "                .weightInit(WeightInit.XAVIER)\n",
    "                .build())\n",
    "        .setInputType(InputType.convolutional(height, width, channels))\n",
    "        .build();\n",
    "\n",
    "UIServer uiServer = UIServer.getInstance();\n",
    "StatsStorage statsStorage = new FileStatsStorage(new File(System.getProperty(\"java.io.tmpdir\"), \"ui-dnn-stats.dl4j\"));\n",
    "uiServer.attach(statsStorage);\n",
    "\n",
    "MultiLayerNetwork model = new MultiLayerNetwork(conf);\n",
    "model.init();\n",
    "model.setListeners(new ScoreIterationListener(10), new StatsListener(statsStorage), new EvaluativeListener(testIter, 1, InvocationType.EPOCH_END));\n",
    "\n",
    "\n",
    "//Train the model and evaluate\n",
    "for (int i = 0; i < numEpochs; i++) {\n",
    "    model.fit(trainIter);\n",
    "    System.out.println(\"********Evaluation Stats*********\");\n",
    "    Evaluation eval = model.evaluate(testIter);\n",
    "    System.out.println(eval.stats());\n",
    "\n",
    "    trainIter.reset();\n",
    "    testIter.reset();\n",
    "}\n",
    "\n",
    "System.out.println(\"********Example finished*********\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional networks\n",
    "\n",
    "Convolutional neural networks are the specialized models that are highly efficient for processing information that can be represented in terms of measurements on a grid. This includes images, which are measurements of brightness on a two-dimensional grid, audio waveforms, which can be regarded as a one-dimensional grid across time, and three-dimensional grid data such as 3-D scans used in medical imaging. For a convolutional network, we use 4-dimensional arrays (known as **feature map**) to keep track of the shape of the image. A feature map is split into several **channels**. Each channel describes how a single type of feature appears across the entire image. The feature map is of shape $m*h*w*c$ where:\n",
    "* $m$ is the number of examples to process together in the same batch,\n",
    "* $h$ is the height of the image,\n",
    "* $w$ is the width of the image, and\n",
    "* $c$ is the number of channels.\n",
    "\n",
    "Now let's create a convolutional neural network and train it on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Evaluation Stats*********\n",
      "\n",
      "Examples labeled as 0 classified by model as 0: 967 times\n",
      "Examples labeled as 0 classified by model as 2: 1 times\n",
      "Examples labeled as 0 classified by model as 5: 2 times\n",
      "Examples labeled as 0 classified by model as 7: 2 times\n",
      "Examples labeled as 0 classified by model as 8: 2 times\n",
      "Examples labeled as 0 classified by model as 9: 6 times\n",
      "Examples labeled as 1 classified by model as 1: 1132 times\n",
      "Examples labeled as 1 classified by model as 3: 1 times\n",
      "Examples labeled as 1 classified by model as 5: 1 times\n",
      "Examples labeled as 1 classified by model as 6: 1 times\n",
      "Examples labeled as 2 classified by model as 0: 1 times\n",
      "Examples labeled as 2 classified by model as 1: 2 times\n",
      "Examples labeled as 2 classified by model as 2: 1015 times\n",
      "Examples labeled as 2 classified by model as 3: 3 times\n",
      "Examples labeled as 2 classified by model as 4: 1 times\n",
      "Examples labeled as 2 classified by model as 5: 1 times\n",
      "Examples labeled as 2 classified by model as 7: 7 times\n",
      "Examples labeled as 2 classified by model as 8: 1 times\n",
      "Examples labeled as 2 classified by model as 9: 1 times\n",
      "Examples labeled as 3 classified by model as 2: 1 times\n",
      "Examples labeled as 3 classified by model as 3: 990 times\n",
      "Examples labeled as 3 classified by model as 5: 8 times\n",
      "Examples labeled as 3 classified by model as 7: 2 times\n",
      "Examples labeled as 3 classified by model as 9: 9 times\n",
      "Examples labeled as 4 classified by model as 4: 975 times\n",
      "Examples labeled as 4 classified by model as 9: 7 times\n",
      "Examples labeled as 5 classified by model as 0: 1 times\n",
      "Examples labeled as 5 classified by model as 2: 1 times\n",
      "Examples labeled as 5 classified by model as 3: 4 times\n",
      "Examples labeled as 5 classified by model as 5: 883 times\n",
      "Examples labeled as 5 classified by model as 7: 1 times\n",
      "Examples labeled as 5 classified by model as 9: 2 times\n",
      "Examples labeled as 6 classified by model as 0: 10 times\n",
      "Examples labeled as 6 classified by model as 1: 3 times\n",
      "Examples labeled as 6 classified by model as 2: 1 times\n",
      "Examples labeled as 6 classified by model as 3: 1 times\n",
      "Examples labeled as 6 classified by model as 4: 16 times\n",
      "Examples labeled as 6 classified by model as 5: 27 times\n",
      "Examples labeled as 6 classified by model as 6: 899 times\n",
      "Examples labeled as 6 classified by model as 8: 1 times\n",
      "Examples labeled as 7 classified by model as 1: 4 times\n",
      "Examples labeled as 7 classified by model as 2: 5 times\n",
      "Examples labeled as 7 classified by model as 3: 4 times\n",
      "Examples labeled as 7 classified by model as 7: 1012 times\n",
      "Examples labeled as 7 classified by model as 9: 3 times\n",
      "Examples labeled as 8 classified by model as 0: 4 times\n",
      "Examples labeled as 8 classified by model as 2: 5 times\n",
      "Examples labeled as 8 classified by model as 3: 2 times\n",
      "Examples labeled as 8 classified by model as 4: 4 times\n",
      "Examples labeled as 8 classified by model as 5: 8 times\n",
      "Examples labeled as 8 classified by model as 6: 1 times\n",
      "Examples labeled as 8 classified by model as 7: 4 times\n",
      "Examples labeled as 8 classified by model as 8: 919 times\n",
      "Examples labeled as 8 classified by model as 9: 27 times\n",
      "Examples labeled as 9 classified by model as 1: 2 times\n",
      "Examples labeled as 9 classified by model as 2: 1 times\n",
      "Examples labeled as 9 classified by model as 3: 1 times\n",
      "Examples labeled as 9 classified by model as 4: 6 times\n",
      "Examples labeled as 9 classified by model as 5: 2 times\n",
      "Examples labeled as 9 classified by model as 7: 4 times\n",
      "Examples labeled as 9 classified by model as 9: 993 times\n",
      "\n",
      "\n",
      "==========================Scores========================================\n",
      " # of classes:    10\n",
      " Accuracy:        0.9785\n",
      " Precision:       0.9786\n",
      " Recall:          0.9781\n",
      " F1 Score:        0.9781\n",
      "Precision, recall & F1: macro-averaged (equally weighted avg. of 10 classes)\n",
      "========================================================================\n",
      "********Example finished*********\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.datavec.api.io.labels.ParentPathLabelGenerator;\n",
    "import org.datavec.api.split.FileSplit;\n",
    "import org.datavec.image.loader.NativeImageLoader;\n",
    "import org.datavec.image.recordreader.ImageRecordReader;\n",
    "import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator;\n",
    "import org.deeplearning4j.eval.Evaluation;\n",
    "import org.deeplearning4j.nn.conf.MultiLayerConfiguration;\n",
    "import org.deeplearning4j.nn.conf.NeuralNetConfiguration;\n",
    "import org.deeplearning4j.nn.conf.Updater;\n",
    "import org.deeplearning4j.nn.conf.inputs.InputType;\n",
    "import org.deeplearning4j.nn.conf.layers.ConvolutionLayer;\n",
    "import org.deeplearning4j.nn.conf.layers.DenseLayer;\n",
    "import org.deeplearning4j.nn.conf.layers.OutputLayer;\n",
    "import org.deeplearning4j.nn.conf.layers.SubsamplingLayer;\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;\n",
    "import org.deeplearning4j.nn.weights.WeightInit;\n",
    "import org.deeplearning4j.optimize.api.InvocationType;\n",
    "import org.nd4j.linalg.activations.Activation;\n",
    "import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;\n",
    "import org.nd4j.linalg.dataset.api.preprocessor.DataNormalization;\n",
    "import org.nd4j.linalg.dataset.api.preprocessor.ImagePreProcessingScaler;\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions;\n",
    "\n",
    "import java.io.File;\n",
    "import java.util.Random;\n",
    "\n",
    "int seed = 123;\n",
    "double learningRate = 0.01;\n",
    "int batchSize = 100;\n",
    "int numEpochs = 1;\n",
    "\n",
    "int height = 28;\n",
    "int width = 28;\n",
    "int channels = 1;\n",
    "int numInput = height * width;\n",
    "int numHidden = 1000;\n",
    "int numOutput = 10;\n",
    "\n",
    "//Prepare data for loading\n",
    "File trainData = new File(\"./assets/mnist_png/training\");\n",
    "FileSplit trainSplit = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, new Random(seed));\n",
    "ParentPathLabelGenerator labelMaker = new ParentPathLabelGenerator(); // use parent directory name as the image label\n",
    "ImageRecordReader trainRR = new ImageRecordReader(height, width, 1, labelMaker);\n",
    "trainRR.initialize(trainSplit);\n",
    "DataSetIterator trainIter = new RecordReaderDataSetIterator(trainRR, batchSize, 1, numOutput);\n",
    "DataNormalization imageScaler = new ImagePreProcessingScaler();\n",
    "imageScaler.fit(trainIter);\n",
    "trainIter.setPreProcessor(imageScaler);\n",
    "\n",
    "\n",
    "File testData = new File(\"./assets/mnist_png/testing\");\n",
    "FileSplit testSplit = new FileSplit(testData, NativeImageLoader.ALLOWED_FORMATS, new Random(seed));\n",
    "ImageRecordReader testRR = new ImageRecordReader(height, width, 1, labelMaker);\n",
    "testRR.initialize(testSplit);\n",
    "DataSetIterator testIter = new RecordReaderDataSetIterator(testRR, batchSize, 1, numOutput);\n",
    "testIter.setPreProcessor(imageScaler);\n",
    "\n",
    "//Build the neural network\n",
    "MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n",
    "            .seed(seed)\n",
    "            .updater(Updater.ADAM)\n",
    "            .weightInit(WeightInit.XAVIER)\n",
    "            .list()\n",
    "            .layer(0, new ConvolutionLayer.Builder(5, 5)\n",
    "                .nIn(channels)\n",
    "                .stride(1, 1)\n",
    "                .nOut(20)\n",
    "                .activation(Activation.IDENTITY)\n",
    "                .build())\n",
    "            .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n",
    "                .kernelSize(2, 2)\n",
    "                .stride(2, 2)\n",
    "                .build())\n",
    "            .layer(2, new ConvolutionLayer.Builder(5, 5)\n",
    "                .stride(1, 1) // nIn need not specified in later layers\n",
    "                .nOut(50)\n",
    "                .activation(Activation.IDENTITY)\n",
    "                .build())\n",
    "            .layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n",
    "                .kernelSize(2, 2)\n",
    "                .stride(2, 2)\n",
    "                .build())\n",
    "            .layer(4, new DenseLayer.Builder().activation(Activation.RELU)\n",
    "                .nOut(500)\n",
    "                .build())\n",
    "            .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n",
    "                .nOut(numOutput)\n",
    "                .activation(Activation.SOFTMAX)\n",
    "                .build())\n",
    "            .setInputType(InputType.convolutionalFlat(height, width, channels)) \n",
    "            .build();\n",
    "\n",
    "\n",
    "MultiLayerNetwork model = new MultiLayerNetwork(conf);\n",
    "model.init();\n",
    "\n",
    "//Train the model and evaluate\n",
    "for (int i = 0; i < numEpochs; i++) {\n",
    "    model.fit(trainIter);\n",
    "    System.out.println(\"********Evaluation Stats*********\");\n",
    "    Evaluation eval = model.evaluate(testIter);\n",
    "    System.out.println(eval.stats());\n",
    "\n",
    "    trainIter.reset();\n",
    "    testIter.reset();\n",
    "}\n",
    "\n",
    "System.out.println(\"********Example finished*********\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now delete the MNIST dataset files as they are no longer required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aima.notebooks.deeplearning.DataUtils;\n",
    "import java.io.File;\n",
    "\n",
    "String DATA_URL = \"http://github.com/myleott/mnist_png/raw/master/mnist_png.tar.gz\";\n",
    "String BASE_PATH = \"./assets\";\n",
    "String localFilePath = BASE_PATH + \"/mnist_png.tar.gz\";\n",
    "\n",
    "File file = new File(localFilePath);\n",
    "file.delete();\n",
    "DataUtils dataUtils = new DataUtils();\n",
    "dataUtils.deleteDir(BASE_PATH + \"/mnist_png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks are the networks that introduce the concept of time i.e they allow us to define the value of some variable $v$ at time step $t$ in terms of the values of this variable at previous time steps. For example, we can define an update rule: $v_{(t)} = f(v_{(t-1)})$ using some function $f$ of our choice. These networks are particularly well suited for sequence processing tasks as they allow us to operate over the **sequences of vectors**: sequences in the input, the output, or in the most general case both. In the last few years, there has been incredible success applying RNNs to a variety of problems such as speech recognition, language modeling, translation, image captioning and the list goes on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll apply RNN to a simple problem of generating text character by character. So, let's start..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed. Sample:\t                                          \n",
      "Epoch 1 completed. Sample:\t                                          \n",
      "Epoch 2 completed. Sample:\te   aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "Epoch 3 completed. Sample:\teequuikk uukuukkuukuukuukuukkuukuukuukuukk\n",
      "Epoch 4 completed. Sample:\tThe    orororororororororororororororororo\n",
      "Epoch 5 completed. Sample:\tThe     o oooooooooooooooooooooooooooooooo\n",
      "Epoch 6 completed. Sample:\tThhhq       o ogg.g.*jupssooo ogg.jmpssooo\n",
      "Epoch 7 completed. Sample:\tTheqqucccc    o o o o o o o o o o o o o o \n",
      "Epoch 8 completed. Sample:\tThequiccck  o o o o o o o o o o o o o o o \n",
      "Epoch 9 completed. Sample:\tThe quick brove o ox jumps ove o o ox jump\n",
      "Epoch 10 completed. Sample:\tThe quick br o  ox jumps ove a ove a ove a\n",
      "Epoch 11 completed. Sample:\tThe quick br a lazy dog.***Tee  uuick br a\n",
      "Epoch 12 completed. Sample:\tThe quick brown fox jumps over a lazy dog.\n",
      "Epoch 13 completed. Sample:\tThe quick brown fox jumps over a lazy dog.\n",
      "Epoch 14 completed. Sample:\tThe quick brown fox jumps over a lazy dog.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.deeplearning4j.nn.conf.MultiLayerConfiguration;\n",
    "import org.deeplearning4j.nn.conf.NeuralNetConfiguration;\n",
    "import org.deeplearning4j.nn.conf.Updater;\n",
    "import org.deeplearning4j.nn.conf.layers.LSTM;\n",
    "import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;\n",
    "import org.deeplearning4j.nn.weights.WeightInit;\n",
    "import org.nd4j.linalg.activations.Activation;\n",
    "import org.nd4j.linalg.api.ndarray.INDArray;\n",
    "import org.nd4j.linalg.api.ops.impl.indexaccum.IMax;\n",
    "import org.nd4j.linalg.dataset.DataSet;\n",
    "import org.nd4j.linalg.factory.Nd4j;\n",
    "import org.nd4j.linalg.api.ops.Op;\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions;\n",
    "\n",
    "import java.util.ArrayList;\n",
    "import java.util.LinkedHashSet;\n",
    "import java.util.List;\n",
    "\n",
    "int seed = 123;\n",
    "int nHidden = 50;\n",
    "int epochs = 15;\n",
    "\n",
    "//Define a sentence to learn\n",
    "//Add a dummy character in beginning so that the RNN learns the complete sentence.\n",
    "char[] LEARNSTRING = \"*The quick brown fox jumps over a lazy dog.\".toCharArray();\n",
    "\n",
    "LinkedHashSet<Character> LEARNSTRING_CHARS = new LinkedHashSet<>();\n",
    "for (char c : LEARNSTRING) LEARNSTRING_CHARS.add(c);\n",
    "List<Character> LEARNSTRING_CHARS_LIST = new ArrayList<>();\n",
    "LEARNSTRING_CHARS_LIST.addAll(LEARNSTRING_CHARS);\n",
    "\n",
    "\n",
    "//Build the neural network\n",
    "MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n",
    "        .seed(seed)\n",
    "        .updater(Updater.ADAM)\n",
    "        .weightInit(WeightInit.XAVIER)\n",
    "        .list()\n",
    "        .layer(0, new LSTM.Builder()\n",
    "                .nIn(LEARNSTRING_CHARS.size())\n",
    "                .nOut(nHidden)\n",
    "                .activation(Activation.TANH)\n",
    "                .build())\n",
    "        .layer(1, new RnnOutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n",
    "                .activation(Activation.SOFTMAX)\n",
    "                .nIn(nHidden)\n",
    "                .nOut(LEARNSTRING_CHARS.size())\n",
    "                .build())\n",
    "        .build();\n",
    "\n",
    "MultiLayerNetwork model = new MultiLayerNetwork(conf);\n",
    "model.init();\n",
    "\n",
    "//Create our training data\n",
    "int[] shape = [1, LEARNSTRING_CHARS_LIST.size(), LEARNSTRING.length]\n",
    "INDArray input = Nd4j.zeros(shape);\n",
    "INDArray labels = Nd4j.zeros(shape);\n",
    "\n",
    "int pos = 0;\n",
    "for (char currChar : LEARNSTRING) {\n",
    "    char nextChar = LEARNSTRING[(pos + 1) % (LEARNSTRING.length)]; //When currChar is the last, take the first character as nextChar.\n",
    "    // Input neuron for current character is 1 at \"pos\"\n",
    "    int[] inputArr = [0, LEARNSTRING_CHARS_LIST.indexOf(currChar), pos];\n",
    "    input.putScalar(inputArr, 1);\n",
    "\n",
    "    // Output neuron for next character is 1 at \"pos\"\n",
    "    int[] labelArr = [0, LEARNSTRING_CHARS_LIST.indexOf(nextChar), pos];\n",
    "    labels.putScalar(labelArr, 1);\n",
    "    pos++;\n",
    "}\n",
    "\n",
    "DataSet trainingData = new DataSet(input, labels);\n",
    "\n",
    "//Train the model and evaluate\n",
    "for (int i = 0; i < epochs; i++) {\n",
    "    model.fit(trainingData);\n",
    "    model.rnnClearPreviousState();\n",
    "\n",
    "    System.out.print(\"Epoch \" + i + \" completed. Sample:\\t\");\n",
    "    //Evaluate\n",
    "    //Put the first character into RNN as an initialisation\n",
    "    int[] testShape = [1, LEARNSTRING_CHARS_LIST.size(), 1]\n",
    "    INDArray testInit = Nd4j.zeros(testShape);\n",
    "    testInit.putScalar(LEARNSTRING_CHARS_LIST.indexOf(LEARNSTRING[0]), 1);\n",
    "\n",
    "    INDArray output = model.rnnTimeStep(testInit);\n",
    "    //output now contains the highest value neuron at such a position which is the index of a character, which the model thinks should come next\n",
    "    //now the model should guess (LEARNSTRING.length - 1) more characters...\n",
    "\n",
    "    for (int j = 0; j < LEARNSTRING.length - 1; j++) {\n",
    "\n",
    "        //First let's process the last output of the model.\n",
    "        int sampledCharacterIndex = Nd4j.getExecutioner().exec(new IMax(output, null, 1),1).getAt(0);\n",
    "        System.out.print(LEARNSTRING_CHARS_LIST.get(sampledCharacterIndex));\n",
    "\n",
    "        //Use the last output as next input\n",
    "        int[] nextInputShape = [1, LEARNSTRING_CHARS_LIST.size(), 1];\n",
    "        INDArray nextInput = Nd4j.zeros(nextInputShape);\n",
    "        nextInput.putScalar(sampledCharacterIndex, 1);\n",
    "        output = model.rnnTimeStep(nextInput);\n",
    "    }\n",
    "    System.out.println();\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Groovy",
   "language": "groovy",
   "name": "groovy"
  },
  "language_info": {
   "codemirror_mode": "groovy",
   "file_extension": ".groovy",
   "mimetype": "",
   "name": "Groovy",
   "nbconverter_exporter": "",
   "version": "2.4.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
